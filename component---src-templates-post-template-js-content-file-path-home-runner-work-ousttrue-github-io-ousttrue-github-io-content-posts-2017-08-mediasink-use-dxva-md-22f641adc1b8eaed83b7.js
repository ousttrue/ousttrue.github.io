"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[9748],{4984:function(e,n,r){r.r(n),r.d(n,{default:function(){return u}});var t=r(1151),c=r(7294);function l(e){const n=Object.assign({p:"p",pre:"pre",code:"code"},(0,t.ah)(),e.components);return c.createElement(c.Fragment,null,c.createElement(n.p,null,"MediaSink で DXVA を使うには。"),"\n",c.createElement(n.p,null,"https://github.com/Microsoft/Windows-classic-samples/tree/master/Samples/DX11VideoRenderer"),"\n",c.createElement(n.p,null,"解読の後半。\nDXVA が何かということについてはぼんやりとしているのだけれど、VideoSample のバッファーに D3D のテクスチャを使うということぽい。"),"\n",c.createElement(n.p,null,"DirectX Surface Buffer"),"\n",c.createElement(n.p,null,"ということは Pipeline のどこかのタイミングで CPU 上の bitmap を CreateTexture して GPU に移動するのだけど、Decoder なり Renderer なりのなるべく上流で GPU に上げた方がうれしいという話。\nID3D11Device を MediaSession に供給する\nPipeline でテクスチャをやりとりするのだからデバイスを共有しましょうと。MediaSession の場合は、レンダラがデバイスを作成して IMFDXGIDeviceManager を公開する。\n公開するのは IMFGetService を通してぽい。\nこの辺。"),"\n",c.createElement(n.pre,null,c.createElement(n.code,{className:"language-c++"},"HRESULT DX11VideoRenderer::CMediaSink::GetService(__RPC__in REFGUID guidService, __RPC__in REFIID riid, __RPC__deref_out_opt LPVOID* ppvObject)\n{\n    HRESULT hr = S_OK;\n\n    if (guidService == MF_RATE_CONTROL_SERVICE)\n    {\n        hr = QueryInterface(riid, ppvObject);\n    }\n    else if (guidService == MR_VIDEO_RENDER_SERVICE)\n    {\n        hr = m_pPresenter->QueryInterface(riid, ppvObject);\n    }\n    else if (guidService == MR_VIDEO_ACCELERATION_SERVICE)\n    {\n        // ここからIMFDXGIDeviceManagerを得る\n        hr = m_pPresenter->GetService(guidService, riid, ppvObject);\n    }\n    else\n    {\n        hr = MF_E_UNSUPPORTED_SERVICE;\n    }\n\n    return hr;\n}\n")),"\n",c.createElement(n.p,null,"実験\nまだ IMFGetService を実装していない VideoRenderer で、\nProcessSample に入ってくる IMFSample から IMFDXGIBuffer が取得できるか試してみよう。"),"\n",c.createElement(n.pre,null,c.createElement(n.code,{className:"language-c++"},"DWORD cBuffers = 0;\nauto hr = pSample->GetBufferCount(&cBuffers);\nif (FAILED(hr))\n{\n    return hr;\n}\n\nMicrosoft::WRL::ComPtr<IMFMediaBuffer> pBuffer;\nif (1 == cBuffers)\n{\n    hr = pSample->GetBufferByIndex(0, &pBuffer);\n}\nelse\n{\n    hr = pSample->ConvertToContiguousBuffer(&pBuffer);\n}\nif (FAILED(hr))\n{\n    return hr;\n}\n\nMicrosoft::WRL::ComPtr<IMFDXGIBuffer> pDXGIBuffer;\nhr = pBuffer.As(&pDXGIBuffer);\nif (FAILED(hr))\n{\n    // ここに来た\n    return hr;\n}\n\nMicrosoft::WRL::ComPtr<ID3D11Texture2D> pTexture2D;\nhr = pDXGIBuffer->GetResource(IID_PPV_ARGS(&pTexture2D));\nif (FAILED(hr))\n{\n    return hr;\n}\n")),"\n",c.createElement(n.p,null,"実験２\nStreamSink に IMFGetService を実装した。"),"\n",c.createElement(n.pre,null,c.createElement(n.code,{className:"language-c++"},"// IMFGetService\nSTDMETHODIMP GetService(__RPC__in REFGUID guidService, __RPC__in REFIID riid, __RPC__deref_out_opt LPVOID* ppvObject)override\n{\n    HRESULT hr = S_OK;\n\n    if (guidService == MR_VIDEO_ACCELERATION_SERVICE)\n    {\n        if (riid == __uuidof(IMFDXGIDeviceManager))\n        {\n            if (NULL != m_pDXGIManager)\n            {\n                *ppvObject = (void*) static_cast<IUnknown*>(m_pDXGIManager);\n                ((IUnknown*)*ppvObject)->AddRef();\n            }\n            else\n            {\n                hr = E_NOINTERFACE;\n            }\n        }\n        else\n        {\n            hr = E_NOINTERFACE;\n        }\n    }\n    else\n    {\n        hr = MF_E_UNSUPPORTED_SERVICE;\n    }\n\n    return hr;\n}\n")),"\n",c.createElement(n.p,null,"IMFSample から ID3D11Texture2D を取得できた。\n上流が、DXVA 化されて Sample のバッファがテクスチャになった。\nどんなテクスチャなのか\nArraySize = 13\nFormat = DXGI_FORMAT_NV12"),"\n",c.createElement(n.p,null,"中身がよくわからぬ。\nDecode された yuv フレームを Swapchain にコピーする"),"\n",c.createElement(n.p,null,"deinterlace\nYUV To RGB\nサイズ調整"),"\n",c.createElement(n.p,null,"等をしてデコード済みのフレームを RGB 画像にする工程。\n２種類の実装がある。"),"\n",c.createElement(n.p,null,"https://github.com/Microsoft/Windows-classic-samples/blob/master/Samples/DX11VideoRenderer/cpp/Presenter.cpp"),"\n",c.createElement(n.p,null,"の以下の部分。"),"\n",c.createElement(n.pre,null,c.createElement(n.code,{className:"language-c++"},"if (m_useXVP)\n{\n    BOOL bInputFrameUsed = FALSE;\n\n    hr = ProcessFrameUsingXVP( pCurrentType, pSample, pTexture2D, rcDest, ppOutputSample, &bInputFrameUsed );\n\n    if (SUCCEEDED(hr) && !bInputFrameUsed)\n    {\n        *pbProcessAgain = TRUE;\n    }\n}\nelse\n{\n    hr = ProcessFrameUsingD3D11( pTexture2D, pEVTexture2D, dwViewIndex, dwEVViewIndex, rcDest, *punInterlaceMode, ppOutputSample );\n\n    // 省略\n}\n")),"\n",c.createElement(n.p,null,"どちらでもだいたい同じ動きになると思う。\nProcessFrameUsingXVP"),"\n",c.createElement(n.p,null,"Video Processor MFT"),"\n",c.createElement(n.p,null,"初期化時に IDXGIDeviceManager を直接渡して DXVA を有効にしている。"),"\n",c.createElement(n.pre,null,c.createElement(n.code,{className:"language-c++"},"hr = CoCreateInstance(CLSID_VideoProcessorMFT, nullptr, CLSCTX_INPROC_SERVER, IID_IMFTransform, (void**)&m_pXVP);\nif (FAILED(hr))\n{\n    break;\n}\n\n// MFTにDirectXを渡す\nhr = m_pXVP->ProcessMessage(MFT_MESSAGE_SET_D3D_MANAGER, ULONG_PTR(m_pDXGIManager));\nif (FAILED(hr))\n{\n    break;\n}\n")),"\n",c.createElement(n.p,null,"Texture の入ったサンプルを処理して、Texture の入ったサンプルに出力できる。\nProcessFrameUsingD3D11\nD3D11VideoDevice を使う。\nこっちの方が手順が長くて大変。\nおそらく、VideoProcessorMFT は D3D11VideoDevice を使って実装していてこちらの方がローレベルなのであろう。\nDecode\nAPI の説明としてはこれ。"),"\n",c.createElement(n.p,null,"Supporting Direct3D 11 Video Decoding in Media Foundation"),"\n",c.createElement(n.p,null,"DX11VideoRenderer サンプルでは、直接使っていない。\nVideo Processing\nDX11VideoRenderer サンプルでは、D3D11VideoDevice を最後の色変換等で使っている。"),"\n",c.createElement(n.p,null,"DXVA Video Processing"),"\n",c.createElement(n.p,null,"Video Process Blit"),"\n",c.createElement(n.p,null,"DXVA2.0+D3D9 のドキュメントぽい。\nD3D11 ではこの関数。"),"\n",c.createElement(n.p,null,c.createElement(n.code,null,"D3D11VideoContext::VideoProcessorBlt")))}var i=function(e){void 0===e&&(e={});const{wrapper:n}=Object.assign({},(0,t.ah)(),e.components);return n?c.createElement(n,e,c.createElement(l,e)):l(e)};r(8678);function a(e){let{data:n,children:r}=e;return c.createElement(c.Fragment,null,c.createElement("h1",null,n.mdx.frontmatter.title),c.createElement(t.Zo,null,r))}function u(e){return c.createElement(a,e,c.createElement(i,e))}},8678:function(e,n,r){r(7294)},1151:function(e,n,r){r.d(n,{Zo:function(){return a},ah:function(){return l}});var t=r(7294);const c=t.createContext({});function l(e){const n=t.useContext(c);return t.useMemo((()=>"function"==typeof e?e(n):{...n,...e}),[n,e])}const i={};function a({components:e,children:n,disableParentContext:r}){let a;return a=r?"function"==typeof e?e({}):e||i:l(e),t.createElement(c.Provider,{value:a},n)}}}]);
//# sourceMappingURL=component---src-templates-post-template-js-content-file-path-home-runner-work-ousttrue-github-io-ousttrue-github-io-content-posts-2017-08-mediasink-use-dxva-md-22f641adc1b8eaed83b7.js.map